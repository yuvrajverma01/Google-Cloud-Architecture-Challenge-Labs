# Perform Foundational Data, ML, and AI Tasks in Google Cloud: Challenge Lab

> Launch the lab [here](https://google.qwiklabs.com/focuses/11044?parent=catalog)

## Your challenge

### Task 1: Run a simple Dataflow job

* Navigation menu > Storage > Browser

* Create a **Storage** Bucket > Enter name as your `GCP Project ID` > Leave others to default > Create

* Go to **BigQuery** > Select `project ID` > Create Dataset > Enter the name as `lab` and click on Create

* Run the following from the Cloud Shell:

```yaml
gsutil cp gs://cloud-training/gsp323/lab.csv .
cat lab.csv
gsutil cp gs://cloud-training/gsp323/lab.schema .
cat lab.schema
```

* Copy and store the last 14 line output from cloud shell

* Now, create a table inside the `lab` dataset and configure it as follows:

1. In **Source** select Google Cloud Storage
2. In **Select file from GCS bucket** and paste `gs://cloud-training/gsp323/lab.csv`
3. In **Table name** type in `customer`

![](https://github.com/yuvrajverma01/Google-Cloud-Architecture-Challenge-Labs/raw/master/media/dataflow_table.png)

* Click on **Create table**

* Go to **Dataflow** > Jobs > Create Job from Template

1. In **Job** name type `lab-transform`
2. **Regional Endpoint** is `us-central1`
3. `IMPORTANT` In **Dataflow Template** choose `Text Files on Cloud Storage to BigQuery` from `Bulk`
4. In **JavaScript UDF path in Cloud Storage** type `gs://cloud-training/gsp323/lab.js`
5. In **JSON path** type `gs://cloud-training/gsp323/lab.schema`
6. In **JavaScript UDF name** type `transform`
7. In **BigQuery output table** type `YOUR_PROJECT:lab.customers`
8. In **Cloud Storage input path** type `gs://cloud-training/gsp323/lab.csv`
9. In **Temporary BigQuery directory** type `gs://YOUR_PROJECT/bigquery_temp`
10. In **Temporary location** type `gs://YOUR_PROJECT/temp`

![](https://github.com/yuvrajverma01/Google-Cloud-Architecture-Challenge-Labs/raw/master/media/jobpage_1.png)

![](https://github.com/yuvrajverma01/Google-Cloud-Architecture-Challenge-Labs/raw/master/media/jobpage_2.png)

* Run the Job.

* Wait for job to get over (this takes 10 - 15 minutes).

### Task 2: Run a simple Dataproc job

* Go to **Dataproc** > Clusters > Create Cluster

![](https://github.com/yuvrajverma01/Google-Cloud-Architecture-Challenge-Labs/raw/master/media/creatingcluster.png)

* Click on the name of `Created Cluster` > Go to **VM Instances** > **SSH** into cluster

* Run the following command: 

```yaml
hdfs dfs -cp gs://cloud-training/gsp323/data.txt /data.txt
```

* Exit the **SSH**

* Submit **Job** > Configure as given:

1. In **Job type** name type `Spark`
2. **Region** is `us-central1`
3. In **Main class or jar** choose `org.apache.spark.examples.SparkPageRank`
4. In **Jar files** type `file:///usr/lib/spark/examples/jars/spark-examples.jar`
5. In **Arguments** type `/data.txt`
6. In **JMax restarts per hour** type `1`

![](https://github.com/yuvrajverma01/Google-Cloud-Architecture-Challenge-Labs/raw/master/media/jobpage_3.png)

* Click on **SUBMIT**

### Task 3: Run a simple Dataprep job

* Go to **Dataprep** > Accept the terms > Login with the same account

* Import Data > Select GCS > Edit > Enter the path as this: `gs://cloud-training/gsp323/runs.csv` > Import and Wrangle

* Watch this ![video](https://www.youtube.com/watch?v=FTnK26oRj8g)

### Task 4: AI

* Create an API Key by going to IAM > Credentials, and export it as `API_KEY` variable in the Cloud Shell. 

```yaml

export API_KEY =[api_key_paste_here]

```

* Create the following JSON file:

```yaml
nano request.json
```

```yaml
{
  "config": {
      "encoding":"FLAC",
      "languageCode": "en-US"
  },
  "audio": {
      "uri":"gs://cloud-training/gsp323/task4.flac"
  }
}
```

* Run the following:

  > Replace `YOUR_PROJECT` with your GCP Project ID.

```yaml
curl -s -X POST -H "Content-Type: application/json" --data-binary @request.json \
"https://speech.googleapis.com/v1/speech:recognize?key=${API_KEY}" > result.json

gsutil cp result.json gs://YOUR_PROJECT-marking/task4-gcs.result
```



